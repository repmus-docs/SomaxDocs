\chapter{Overview}\label{sec:3-overview}
This report describes the theoretical model of the Somax 2 environment, which serves as a basis for the implementation of the system as described in \cite{somaxsoftware2021}. Somax is an interactive system which improvises around a musical material, aiming to provide a stylistically coherent improvisation while in real-time listening to and adapting to input from a musician. The system is trained on some musical material selected by the user, from which it constructs a corpus that will serve as a basis for the improvisation. The idea is that Somax should serve as a co-creative agent in the improvisational process, where the system after some initial tuning is able to listen and adapt to the musician in a self-sufficient manner.

This system, of which earlier versions have been presented in \cite{bonnasse-gahot_update_2014} and \cite{borg_2019}, generates musical material based on an external, already existing material. The principle through which the system generates new material is similar to that of concatenative synthesis, which in turn is to some extent alike that of granular synthesis, where very short pieces of audio are sampled from a preselected material and recombined. In this system, however, the grains (or «slices», as they will be denoted from here on) are much longer than in a granular synthesizer - either the duration of a beat or the length between two note onsets - and the output selection is based on listening to a second, external input, and the system is continuously selecting the most suitable slice using statistical machine learning. In other words, the system is improvising around a musical material and in real-time adapting to a musician (or any other sound source). 

In practice, this behaviour is realized by learning the sampled material, which may be either audio or MIDI, in multiple layers, where each layer listens to a single feature (or «trait», as they will be denoted from here on)  of the material, for example pitch, chroma, MFCC:s, velocity, etc. At runtime, each layer then listens to the corresponding trait of a second external audio and/or MIDI source and continuously matches this to the learned material, generating activations in the memory where matches are found. The output is then selected from the point in the memory with the most activity, after the activities in all layers has been merged and scaled. The activities generated at earlier points in time also remain in the memory for some time, thus impacting future time steps and with that simulating a short-term memory with respect to the original material.

The process of learning the sampled material or «constructing a corpus» will be described in section \ref{sec:3-corpus}. The listening, from here on denoted as «influencing», is described in section \ref{sec:3-influence}, and the generation of output, which for practical reasons is decoupled from the influencing process, is described in section \ref{sec:3-generate}. Section \ref{sec:5-transforms} introduces the concept of transforms and its implications for the Somax model, and finally section \ref{sec:6-model-improvements} introduces a number of user-controller parametric filters to allow further control of the output.


